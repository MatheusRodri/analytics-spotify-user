{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd2039fc-9f2a-4aa6-814b-3d5c70258871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ‚öôÔ∏è Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7ee4fca-c9b8-4f74-9d49-96bdf36a6efe",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Instala bibliotecas n√£o instaladas por padr√£o"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: asttokens==3.0.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 1)) (3.0.1)\n",
      "Requirement already satisfied: colorama==0.4.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 2)) (0.4.6)\n",
      "Requirement already satisfied: comm==0.2.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 3)) (0.2.3)\n",
      "Requirement already satisfied: debugpy==1.8.20 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 4)) (1.8.20)\n",
      "Requirement already satisfied: decorator==5.2.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 5)) (5.2.1)\n",
      "Requirement already satisfied: executing==2.2.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 6)) (2.2.1)\n",
      "Requirement already satisfied: ipykernel==7.2.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 7)) (7.2.0)\n",
      "Requirement already satisfied: ipython==9.10.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 8)) (9.10.0)\n",
      "Requirement already satisfied: ipython_pygments_lexers==1.1.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 9)) (1.1.1)\n",
      "Requirement already satisfied: jedi==0.19.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 10)) (0.19.2)\n",
      "Requirement already satisfied: jupyter_client==8.8.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 11)) (8.8.0)\n",
      "Requirement already satisfied: jupyter_core==5.9.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 12)) (5.9.1)\n",
      "Requirement already satisfied: matplotlib-inline==0.2.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 13)) (0.2.1)\n",
      "Requirement already satisfied: nest-asyncio==1.6.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 14)) (1.6.0)\n",
      "Requirement already satisfied: numpy==2.4.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 15)) (2.4.2)\n",
      "Requirement already satisfied: packaging==26.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 16)) (26.0)\n",
      "Requirement already satisfied: pandas==3.0.1 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 17)) (3.0.1)\n",
      "Requirement already satisfied: parso==0.8.6 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 18)) (0.8.6)\n",
      "Requirement already satisfied: platformdirs==4.9.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 19)) (4.9.2)\n",
      "Requirement already satisfied: prompt_toolkit==3.0.52 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 20)) (3.0.52)\n",
      "Requirement already satisfied: psutil==7.2.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 21)) (7.2.2)\n",
      "Requirement already satisfied: pure_eval==0.2.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 22)) (0.2.3)\n",
      "Collecting py4j==0.10.9.9 (from -r requirements.txt (line 23))\n",
      "  Obtaining dependency information for py4j==0.10.9.9 from https://files.pythonhosted.org/packages/bd/db/ea0203e495be491c85af87b66e37acfd3bf756fd985f87e46fc5e3bf022c/py4j-0.10.9.9-py2.py3-none-any.whl.metadata\n",
      "  Using cached py4j-0.10.9.9-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Requirement already satisfied: Pygments==2.19.2 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 24)) (2.19.2)\n",
      "Collecting pyspark==4.1.1 (from -r requirements.txt (line 25))\n",
      "  Using cached pyspark-4.1.1-py2.py3-none-any.whl\n",
      "Requirement already satisfied: python-dateutil==2.9.0.post0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 26)) (2.9.0.post0)\n",
      "Requirement already satisfied: pyzmq==27.1.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 27)) (27.1.0)\n",
      "Requirement already satisfied: six==1.17.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 28)) (1.17.0)\n",
      "Requirement already satisfied: stack-data==0.6.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 29)) (0.6.3)\n",
      "Requirement already satisfied: tornado==6.5.4 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 30)) (6.5.4)\n",
      "Requirement already satisfied: traitlets==5.14.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 31)) (5.14.3)\n",
      "Requirement already satisfied: typing_extensions==4.15.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 32)) (4.15.0)\n",
      "Requirement already satisfied: tzdata==2025.3 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 33)) (2025.3)\n",
      "Requirement already satisfied: wcwidth==0.6.0 in .\\.venv\\lib\\site-packages (from -r requirements.txt (line 34)) (0.6.0)\n",
      "Using cached py4j-0.10.9.9-py2.py3-none-any.whl (203 kB)\n",
      "Installing collected packages: py4j, pyspark\n",
      "  Attempting uninstall: py4j\n",
      "    Found existing installation: py4j 0.10.9.7\n",
      "    Uninstalling py4j-0.10.9.7:\n",
      "      Successfully uninstalled py4j-0.10.9.7\n",
      "  Attempting uninstall: pyspark\n",
      "    Found existing installation: pyspark 3.5.3\n",
      "    Uninstalling pyspark-3.5.3:\n",
      "      Successfully uninstalled pyspark-3.5.3\n",
      "Successfully installed py4j-0.10.9.9 pyspark-4.1.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "delta-spark 3.0.0 requires pyspark<3.6.0,>=3.5.0, but you have pyspark 4.1.1 which is incompatible.\n",
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 26.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries\n",
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "715cc12e-b9e9-4d8a-a9ee-5ed53470d473",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Importe das bibliotecas"
    }
   },
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import current_timestamp,col, dayofweek,when,hour,from_utc_timestamp, concat,year,month,lit,lower,to_date,  sum as _sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Spark session not found\n"
     ]
    }
   ],
   "source": [
    "# Checking environment ( Local or Databricks )\n",
    "try:\n",
    "    spark\n",
    "    print(\"‚úÖ Spark session already exists.\")\n",
    "    IS_DATABRICKS = True\n",
    "except NameError:\n",
    "    print(\"‚ùå Spark session not found\")\n",
    "    IS_DATABRICKS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating Spark session for local environment\n",
    "if not IS_DATABRICKS:\n",
    "    try:\n",
    "        spark = SparkSession.builder\\\n",
    "            .appName(\"Analytics_Spotify\")\\\n",
    "            .master(\"local[*]\")\\\n",
    "            .config(\"spark.driver.memory\", \"4g\")\\\n",
    "            .getOrCreate()\n",
    "        print(\"‚úÖ Spark session created successfully.\")\n",
    "    except ImportError:\n",
    "        print(\"‚ùå Failed to create Spark session. Please ensure PySpark is installed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f76f80b8-8398-474d-858e-117c83ee50e6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Defini vari√°veis"
    }
   },
   "outputs": [],
   "source": [
    "# Define constants for file paths and table names\n",
    "\n",
    "## Databricks\n",
    "PATH_ORIGIN_DATABRICKS = \"/Volumes/sandbox_prd/raw_layer/files/spotify/me/extended/Streaming_History_*.json\"\n",
    "NAME_TABLE_BRONZE_DATABRICKS = \"sandbox_prd.bronze_layer.streaming_history_user_spotify\"\n",
    "NAME_TABLE_SILVER_DATABRICKS = \"sandbox_prd.silver_layer.streaming_history_user_spotify\"\n",
    "\n",
    "## Local\n",
    "PATH_ORIGIN_LOCAL = \"data/extended/Streaming_History_*.json\"\n",
    "NAME_TABLE_BRONZE_LOCAL = \"bronze_streaming_history_user_spotify\"\n",
    "NAME_TABLE_SILVER_LOCAL = \"silver_streaming_history_user_spotify\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e3947b0c-a8a8-4c63-94ab-b3a28c10ecd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•â Bronze Layer\n",
    "Reading JSON files and writing to Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c7d3229d-8b8d-4c3c-be6e-49c61567d450",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Leitura dos arquivos JSON (Bronze Layer)"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Reading JSON files (Inferring Schema)\n",
    "if IS_DATABRICKS:\n",
    "    print(\"Is Databricks\")\n",
    "    df_input = (spark.read\n",
    "                .format(\"json\")\n",
    "                .option(\"multiline\", \"true\") \n",
    "                .option(\"inferSchema\", \"true\") \n",
    "                .load(PATH_ORIGIN_DATABRICKS))\n",
    "else:\n",
    "    print(\"Is Local\")\n",
    "    df_input = (spark.read\n",
    "                .format(\"json\")\n",
    "                .option(\"multiline\", \"true\") \n",
    "                .option(\"inferSchema\", \"true\") \n",
    "                .load(PATH_ORIGIN_LOCAL))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6aa9130d-317c-4588-a270-1f182cd35779",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Enriquecimento dos dados com metadados"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Enriching data with Unity Catalog metadata\n",
    "if IS_DATABRICKS:\n",
    "    df_bronze = df_input.select(\n",
    "    \"*\", \n",
    "    current_timestamp().alias(\"dt_ingestion\"), \n",
    "    col(\"_metadata.file_path\").alias(\"source_file\") \n",
    "    )\n",
    "else:\n",
    "    df_bronze = df_input.select(\n",
    "    \"*\", \n",
    "    current_timestamp().alias(\"dt_ingestion\"), \n",
    "    lit(\"local_file\").alias(\"source_file\") \n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c6d99f35-e518-45a5-bee4-1ce2dd48e347",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Grava√ß√£o dos dados na tabela Delta"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Writing data to Delta table (Schema Evolution)\n",
    "if IS_DATABRICKS:\n",
    "    (df_bronze.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\")              \n",
    "        .option(\"mergeSchema\", \"true\")  \n",
    "        .saveAsTable(NAME_TABLE_BRONZE)\n",
    "    )\n",
    "    print(f\"‚úÖ Loaded table: {NAME_TABLE_BRONZE}\")\n",
    "else:\n",
    "    df_bronze.coalesce(1).write.format(\"csv\").mode(\"overwrite\").option(\"header\", \"true\").save(NAME_TABLE_BRONZE)\n",
    "    print(f\"‚úÖ Loaded CSV: {NAME_TABLE_BRONZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "erro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7e04ff9f-80ef-4670-abff-f3caa69cc56d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## üîç Quality Check\n",
    "\n",
    "Performing quality checks on the Bronze layer data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10774c29-54e6-48c4-883b-fe011d56e6be",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check de Volume e Origem (SQL)"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Valida√ß√£o 1: Volume de dados por arquivo de origem\n",
    "SELECT \n",
    "    source_file,\n",
    "    count(*) as total_linhas\n",
    "FROM sandbox_prd.bronze_layer.streaming_history_user_spotify\n",
    "GROUP BY source_file\n",
    "ORDER BY source_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d4aaa93b-9417-4a97-a166-ceb1a98833ad",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Valida√ß√£o 2: Per√≠odo dos dados (Min e Max)\n",
    "SELECT \n",
    "    min(ts) as primeira_reproducao,\n",
    "    max(ts) as ultima_reproducao,\n",
    "    count(*) as total_geral\n",
    "FROM sandbox_prd.bronze_layer.streaming_history_user_spotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16bdc079-fd41-44d6-936b-6dff66e8487e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Valida√ß√£o 3: Verificando consist√™ncia dos campos principais\n",
    "df_check = spark.read.table(\"sandbox_prd.bronze_layer.streaming_history_user_spotify\")\n",
    "\n",
    "# Conta quantos nulos existem nas colunas chave\n",
    "df_check.select(\n",
    "    _sum(col(\"master_metadata_track_name\").isNull().cast(\"int\")).alias(\"nulos_track_name\"),\n",
    "    _sum(col(\"master_metadata_album_artist_name\").isNull().cast(\"int\")).alias(\"nulos_artist_name\"),\n",
    "    _sum(col(\"ts\").isNull().cast(\"int\")).alias(\"nulos_timestamp\")\n",
    ").display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d6ad707-77a3-43ac-b6a7-16a845395338",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## ü•à Silver Layer\n",
    "\n",
    "Tratamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd37a9cb-a102-491f-8565-25df95c0b8d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Remove colunas desness√°rias"
    }
   },
   "outputs": [],
   "source": [
    "# L√™ a base bronze\n",
    "df_bronze = spark.read.table(\"sandbox_prd.bronze_layer.streaming_history_user_spotify\")\n",
    "\n",
    "# Listas colunas para remover \n",
    "drop_columns = [\n",
    "    \"audiobook_chapter_title\",\n",
    "    \"audiobook_chapter_uri\",\n",
    "    \"audiobook_title\",\n",
    "    \"audiobook_uri\",\n",
    "    \"conn_country\",\n",
    "    \"incognito_mode\",\n",
    "    \"ip_addr\",\n",
    "    \"dt_ingestion\",\n",
    "    \"source_file\"\n",
    "]\n",
    "\n",
    "# Remove as colunas desnecess√°rias\n",
    "df_silver = df_bronze.drop(*drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8ad11ca-473a-4838-a7e0-a3cf987cd565",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Altera nome e o tipo das colunas"
    }
   },
   "outputs": [],
   "source": [
    "v_hora_brasil = hour(from_utc_timestamp(col(\"ts\"), \"America/Sao_Paulo\"))\n",
    "\n",
    "df_silver = df_silver.select(\n",
    "    # Renomia e alteara o tipo da coluna\n",
    "    col(\"episode_name\").alias(\"nm_episode_name\"),\n",
    "    col(\"episode_show_name\").alias(\"nm_episode_show_name\"),\n",
    "    col(\"master_metadata_album_album_name\").alias(\"nm_album_name\"),\n",
    "    col(\"master_metadata_album_artist_name\").alias(\"nm_artist_name\"),\n",
    "    col(\"master_metadata_track_name\").alias(\"nm_track_name\"),\n",
    "    col(\"ms_played\").alias(\"qt_played_ms\"),\n",
    "    col(\"offline\").alias(\"fl_offline\"),\n",
    "    col(\"offline_timestamp\").alias(\"ts_offline\"),\n",
    "    col(\"platform\").alias(\"ds_platform\"),\n",
    "    col(\"reason_end\").alias(\"ds_reason_end\"),\n",
    "    col(\"reason_start\").alias(\"ds_reason_start\"),\n",
    "    col(\"shuffle\").alias(\"fl_shuffle\"),\n",
    "    col(\"skipped\").alias(\"fl_skipped\"),\n",
    "    col(\"ts\").cast(\"Timestamp\").alias(\"ts_streaming\"),\n",
    "    col(\"spotify_episode_uri\").alias(\"ds_spotify_episode_uri\"),\n",
    "    col(\"spotify_track_uri\").alias(\"ds_spotify_track_uri\"),\n",
    "    v_hora_brasil.alias(\"nr_hora_brasil\"),\n",
    "\n",
    "    # Cria coluna ano m√™s\n",
    "    concat(\n",
    "        year(col(\"ts\")).cast(\"String\"),\n",
    "        lit(\"-\"),\n",
    "        month(col(\"ts\")).cast(\"String\")\n",
    "    ).alias(\"dt_ano_mes\"),\n",
    "\n",
    "    # Cria coluna dura√ß√£o em segundos\n",
    "    (col(\"ms_played\")/1000).cast(\"Int\").alias(\"ts_duration_seconds\"),\n",
    "\n",
    "    # Cria coluna de minutos\n",
    "    (col(\"ms_played\")/1000/60).cast(\"Int\").alias(\"ts_duration_minutes\"),\n",
    "\n",
    "    # Cria coluna de dia da semana\n",
    "    when(dayofweek(col(\"ts\")) == 1, \"Domingo\")\n",
    "        .when(dayofweek(col(\"ts\")) == 2, \"Segunda-feira\")\n",
    "        .when(dayofweek(col(\"ts\")) == 3, \"Ter√ßa-feira\")\n",
    "        .when(dayofweek(col(\"ts\")) == 4, \"Quarta-feira\")\n",
    "        .when(dayofweek(col(\"ts\")) == 5, \"Quinta-feira\")\n",
    "        .when(dayofweek(col(\"ts\")) == 6, \"Sexta-feira\")\n",
    "        .when(dayofweek(col(\"ts\")) == 7, \"S√°bado\")\n",
    "        .alias(\"ds_day_of_week\"),\n",
    "    \n",
    "    # Cria coluna per√≠odo do dia\n",
    "    when(v_hora_brasil < 6, \"Madrugada\")\n",
    "        .when(v_hora_brasil < 12, \"Manh√£\")\n",
    "        .when(v_hora_brasil < 18, \"Tarde\")\n",
    "        .otherwise(\"Noite\").alias(\"ds_periodo_dia\"),\n",
    "\n",
    "    # Cria coluna ordem do per√≠odo do dia\n",
    "    when(v_hora_brasil < 6, 1)\n",
    "    .when(v_hora_brasil < 12, 2)\n",
    "    .when(v_hora_brasil < 18, 3)\n",
    "    .otherwise(4).alias(\"nr_ordem_periodo\"),\n",
    "\n",
    "\n",
    "    # Cria coluna tipo do inicio\n",
    "    when(col(\"reason_start\") == \"trackdone\", \"Reprodu√ß√£o Autom√°tica\")\n",
    "    .when(col(\"reason_start\") == \"clickrow\", \"Sele√ß√£o Manual\")\n",
    "    .when(col(\"reason_start\") == \"appload\", \"Retomada App\")\n",
    "    .when(col(\"reason_start\") == \"playbtn\", \"Bot√£o Play\")\n",
    "    .when(col(\"reason_start\").isin(\"fwdbtn\", \"backbtn\"), \"Navega√ß√£o (Pular/Voltar)\")\n",
    "    .when(col(\"reason_start\") == \"remote\", \"Controle Externo\")\n",
    "    .otherwise(\"Outros\").alias(\"ds_tipo_inicio\"),\n",
    "\n",
    "    # Cria coluna device_type\n",
    "    when(lower(col(\"platform\")).contains(\"android\"), \"Android\")\n",
    "    .when(lower(col(\"platform\")).contains(\"ios\"), \"iOS\")\n",
    "    .when(lower(col(\"platform\")).contains(\"web\"), \"Web\")\n",
    "    .when(lower(col(\"platform\")).contains(\"windows\"), \"Windows\")\n",
    "    .when(lower(col(\"platform\")).contains(\"mac\"), \"Mac\")\n",
    "    .when(lower(col(\"platform\")).contains(\"linux\"), \"Linux\")\n",
    "    .when(lower(col(\"platform\")).contains(\"tv\"), \"TV\")\n",
    "    .when(lower(col(\"platform\")).contains(\"echo_show_5\"), \"Echo_Show\")\n",
    "    .when(lower(col(\"platform\")).contains(\"other\"), \"Outros\")\n",
    "    .otherwise(\"N√£o identificado\").alias(\"ds_device_type\"),\n",
    "\n",
    "\n",
    "    # Cria coluna de link de musica clicavel\n",
    "   when(col(\"master_metadata_track_name\").isNotNull(), \n",
    "         concat(lit(\"https://open.spotify.com/track/\"), col(\"spotify_track_uri\"))\n",
    "    )\n",
    "    .when(col(\"master_metadata_album_album_name\").isNotNull(), \n",
    "          concat(lit(\"https://open.spotify.com/album/\"), col(\"spotify_track_uri\"))\n",
    "    )\n",
    "    .when(col(\"episode_show_name\").isNotNull(), \n",
    "          concat(lit(\"https://open.spotify.com/episode/\"), col(\"spotify_track_uri\"))\n",
    "    )\n",
    "    .otherwise(lit(\"N√£o identificado\")).alias(\"ds_link_musica\"),\n",
    "    to_date(col(\"ts\")).alias(\"dt_referencia\"),\n",
    "\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b879a11e-ee1c-46ec-bf31-dace8fa97e0a",
     "showTitle": true,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{\"ds_link_musica\":{\"format\":{\"preset\":\"string-preset-url\",\"locale\":\"en\"}}}},\"syncTimestamp\":1770252153272}",
       "filterBlob": "{\"version\":1,\"filterGroups\":[{\"enabled\":true,\"filterGroupId\":\"fg_e600fbd8\",\"op\":\"OR\",\"filters\":[{\"filterId\":\"f_b080784\",\"enabled\":true,\"columnId\":\"ds_platform\",\"dataType\":\"string\",\"filterType\":\"eq\"}],\"local\":false,\"updatedAt\":1770251217359}],\"syncTimestamp\":1770251217359}",
       "queryPlanFiltersBlob": "[]",
       "tableResultIndex": 0
      }
     },
     "title": "Cria novas colunas"
    }
   },
   "outputs": [],
   "source": [
    "display(df_silver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1af0789a-b3b5-467b-83d5-0319fe52ba5e",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Salva base silver"
    }
   },
   "outputs": [],
   "source": [
    "(df_silver.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"overwrite\")              \n",
    "    .option(\"mergeSchema\", \"true\")  \n",
    "    .saveAsTable(NOME_TABELA_DESTINO_SILVER)\n",
    ")\n",
    "\n",
    "print(f\"‚úÖ Carga conclu√≠da com sucesso em: {NOME_TABELA_DESTINO_SILVER}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8495952617057633,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "main",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": ".venv (3.11.5)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
